---
title: "Machine Learning -Coursera Project"
author: "Prashant Athavale"
date: "November 23, 2014"
output: html_document
---
<font color="#00005F" size="3" face="times"><b>Background</b></font>

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to accurately predict the classification variable "classe".


<font color="#00005F" size="3" face="times"><b>Data</b></font>


The training data for this project are available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
The test data are available here:https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.

<font color="#00005F" size="3" face="times"><b>Methodology: How the model was built?</b></font>

STEP 1: The training data had many missing values, as well as, NA values. This was one of the main difficulty in this project. These values were deleted. Moreover, first few values related to timestamps, names, etc. These values were not used in the analysis. These were manually removed after inspection.

```{r, echo=TRUE}
# cat("\014")
# rm(list=ls());
# library(caret); library(randomForest); 
# training<-read.csv("pml-training-small.csv");
# testing<-read.csv("pml-testing-small.csv");

# trainingSet<- training[sample(1:nrow(training), 10000, replace=FALSE),];
# trainingSet<-trainingSet[,-c(1:7)];
# testingSet<-testing[,-c(1:7)];
```

STEP 2: SCALABILITY

The dataset was further reduced to increase speed, as my laptop was quiet incapable of handling the computations. 



STEP 3: CROSS-VALIDATION

The trainig dataset was further divided into 70 % for training and 30% for testing for crossvalidation.
Cross validation was done with method ="cv" with number=3.
```{r, echo=TRUE}
# inTrain <- createDataPartition (y=trainingSet$classe, p=0.7, list = FALSE);
# train70 <- trainingSet[inTrain,];
# test30 <- trainingSet[-inTrain,];
# ctrl = trainControl(method = "cv", number = 3);
```

STEP 4: OUT OF SAMPLE ERROR =(1 - Overall Accuracy) ~ 0.2

The output is a categorical (factor) variable. The fastest was to predict is to convert it to numeric variable and use "glm" with "pca" preprocessing. But it was found that the model is not very accurate using the Accuracy from the confusion Matrix. 

It was found that the Random Forest, or Boosted Regression Model worked better. All these methods were tried and based on the least out of sample error of Random Forest method was finally chosen.

```{r, echo=TRUE}
# modFit1 <- train(classe ~., method="gbm", trControl=ctrl, preProcess="pca", data=train70);
# pred1<-predict(modFit1, testingSet);
# print(pred1)

# modFit2 <- train(classe ~., method="rf", trControl=ctrl, data=train70);
# pred2<-predict(modFit2, testingSet);
# print(pred2)

# confusionMatrix(pred1, newtest$classe);
# confusionMatrix(pred2, newtest$classe);
```

<b>STEP 5: THE PREDICTION</b>
Based on the Random Forest model, the prediction obtained was as follows.
```{r, echo=TRUE}
# pred2<-predict(modFit2, testingSet);
# > print(pred2)
# [1] B A B A A E D B A A B C B A E E A B B B
# Levels: A B C D E
```
